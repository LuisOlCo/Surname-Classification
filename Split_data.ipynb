{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before spliting any dataset is neccesary to make some data processing, such us turning upper case letters to lower case so the model consider them as the same character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_surname(text):\n",
    "    text = text.lower()\n",
    "    #text = re.sub(r'([.,!?])',r' \\1 ', text)\n",
    "    #text = re.sub(r'[^a-zA-z.,!?]+', r' ', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('surnames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(data)):\n",
    "    data.iloc[row,0] = preprocess_surname(data.iloc[row,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to transform the nationalities into one-hot-encoded vectors, in order to make easier the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_labels(data):\n",
    "    all_nations = sorted(list(set(data.loc[:,'nationality'])))\n",
    "    for i in range(len(data.iloc[:,1])):\n",
    "        idx = all_nations.index(data.iloc[i,1])\n",
    "        encoded_sample = np.zeros(len(all_nations))\n",
    "        encoded_sample[idx] = 1\n",
    "        data.iloc[i,1] = encoded_sample\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_samples_category(data):\n",
    "    category_dict = {}\n",
    "    for i in range(len(data)):\n",
    "        if data.iloc[i,1] not in category_dict:\n",
    "            category_dict.update({data.iloc[i,1]:1})\n",
    "        else:\n",
    "            category_dict[data.iloc[i,1]] += 1\n",
    "    \n",
    "    sort_category_dict = {}\n",
    "    for i in sorted(category_dict.keys()):\n",
    "        sort_category_dict.update({i:category_dict[i]})\n",
    "    return sort_category_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a Dataframe and preprocessed split it with the given proportions\n",
    "def split_data(data,label,train_percentage,test_percentage,validation_percentage):\n",
    "\n",
    "\n",
    "    training_set = pd.DataFrame(columns = data.columns)\n",
    "    validation_set = pd.DataFrame(columns = data.columns)\n",
    "    test_set = pd.DataFrame(columns = data.columns)\n",
    "    \n",
    "    # we sort them by the column of the label so it's going to be easier to classify train, test and validation dataset\n",
    "    # without taking that much memory\n",
    "    data = data.sort_values(by=[label])\n",
    "    \n",
    "    \n",
    "    # We get the number of samples that we have fpr each category with the function number_samples_category,\n",
    "    category_dict = number_samples_category(data)\n",
    "    labels = category_dict.keys()\n",
    "\n",
    "    count = 0\n",
    "    for label in labels:\n",
    "        \n",
    "        total_samples = category_dict[label]\n",
    "        num_training_samples = int(train_percentage*total_samples)\n",
    "        num_test_samples = int(test_percentage*total_samples)\n",
    "        num_validation_samples = total_samples - num_training_samples - num_test_samples\n",
    "        \n",
    "        \n",
    "        training_set = training_set.append(data.iloc[count:count+num_training_samples, :])\n",
    "        count = count+num_training_samples\n",
    "        \n",
    "        test_set = test_set.append(data.iloc[count:count+num_test_samples, :])\n",
    "        count = count + num_test_samples\n",
    "        \n",
    "        validation_set = validation_set.append(data.iloc[count:count+num_validation_samples, :])\n",
    "\n",
    "       \n",
    "        count = num_validation_samples + count\n",
    "\n",
    "    \n",
    "    # Shuffle the dataset after classifying into the train, test and validation datasets\n",
    "    training_set = training_set.sample(frac=1)\n",
    "    test_set = test_set.sample(frac=1)\n",
    "    validation_set = validation_set.sample(frac=1)\n",
    "   \n",
    "    \n",
    "    return training_set, test_set, validation_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First, we are going to explore the data and see how balanced are the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 1603,\n",
       " 'Chinese': 220,\n",
       " 'Czech': 414,\n",
       " 'Dutch': 236,\n",
       " 'English': 2972,\n",
       " 'French': 229,\n",
       " 'German': 576,\n",
       " 'Greek': 156,\n",
       " 'Irish': 183,\n",
       " 'Italian': 600,\n",
       " 'Japanese': 775,\n",
       " 'Korean': 77,\n",
       " 'Polish': 120,\n",
       " 'Portuguese': 55,\n",
       " 'Russian': 2373,\n",
       " 'Scottish': 75,\n",
       " 'Spanish': 258,\n",
       " 'Vietnamese': 58}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_samples_category(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Clearly unbalanced, we are going to split the dataset proportionally into training, test and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remark which column name has the labels for the dataset, in this case is the column named nationality\n",
    "label ='nationality'\n",
    "training_set,test_set,validation_set = split_data(data,label,0.7,0.15,0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training_set = encoding_labels(training_set)\n",
    "test_set = encoding_labels(test_set)\n",
    "validation_set = encoding_labels(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.to_csv('train.csv',index=False)\n",
    "test_set.to_csv('test.csv',index=False)\n",
    "validation_set.to_csv('validation.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
